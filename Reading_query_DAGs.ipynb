{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12fa052b-65ab-42fe-92e9-866a91ab0479",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_path = \"E:\\\\Spark\\\\spark-3.2.0-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(spark_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e02171b5-8b8d-4b81-8826-2fb3ee1a8a24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.driver.memory\", \"10g\")\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"268435456\")\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Topics </h1>\n",
    "\n",
    "1. Reading Files (parquet)\n",
    "2. Narrow Operations\n",
    "   - `filter`\n",
    "   - `withColumn`: adding/modifying a column\n",
    "   - `select`: selecting relevant columns\n",
    "3. Wide Operations\n",
    "   - Joins\n",
    "     - Sort-Merge Join\n",
    "     - Broadcast Join\n",
    "   - GroupBy\n",
    "     - `count` & `sum`\n",
    "     - `countDistinct`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_file = \"E:/PySpark/data/transactions.parquet\"\n",
    "df_transactions = spark.read.parquet(transactions_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transactions.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+-----+------------+\n",
      "|cust_id   |start_date|end_date  |txn_id         |date      |year|month|day|expense_type |amt  |city        |\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+-----+------------+\n",
      "|CCWG6X0ANH|2010-07-01|2019-10-01|TXCC7I6BMRX3XIT|2015-06-27|2015|6    |27 |Entertainment|35.18|denver      |\n",
      "|CCWG6X0ANH|2010-07-01|2019-10-01|TGW6THZKM8C6SUF|2016-08-02|2016|8    |2  |Entertainment|58.15|new_york    |\n",
      "|CCWG6X0ANH|2010-07-01|2019-10-01|TOLMMCXS90H2K31|2014-09-17|2014|9    |17 |Groceries    |43.92|boston      |\n",
      "|CCWG6X0ANH|2010-07-01|2019-10-01|TXLH0P8JII96Q7U|2012-10-05|2012|10   |5  |Entertainment|35.2 |philadelphia|\n",
      "|CCWG6X0ANH|2010-07-01|2019-10-01|TP6RDK6P58M2RO4|2015-10-21|2015|10   |21 |Entertainment|47.44|chicago     |\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+-----+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transactions.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_file = \"E:/PySpark/data/customers.parquet\"\n",
    "df_customers = spark.read.parquet(customers_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+---+------+----------+-----+-----------+\n",
      "|cust_id   |name         |age|gender|birthday  |zip  |city       |\n",
      "+----------+-------------+---+------+----------+-----+-----------+\n",
      "|C007YEYTX9|Aaron Abbott |34 |Female|7/13/1991 |97823|boston     |\n",
      "|C00B971T1J|Aaron Austin |37 |Female|12/16/2004|30332|chicago    |\n",
      "|C00WRSJF1Q|Aaron Barnes |29 |Female|3/11/1977 |23451|denver     |\n",
      "|C01AZWQMF3|Aaron Barrett|31 |Male  |7/9/1998  |46613|los_angeles|\n",
      "|C01BKUFRHA|Aaron Becker |54 |Male  |11/24/1979|40284|san_diego  |\n",
      "+----------+-------------+---+------+----------+-----+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_customers.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Narrow Transformations\n",
    "- `filter` rows where `city='boston'`\n",
    "- `add` a new column: adding `first_name` and `last_name`\n",
    "- `alter` an exisitng column: adding 5 to `age` column\n",
    "- `select` relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_narrow_transform = (\n",
    "    df_customers\n",
    "    .filter(F.col(\"city\") == \"boston\")\n",
    "    .withColumn(\"first_name\", F.split(\"name\", \" \").getItem(0))\n",
    "    .withColumn(\"last_name\", F.split(\"name\", \" \").getItem(1))\n",
    "    .withColumn(\"age\", F.col(\"age\") + F.lit(5))\n",
    "    .select(\"cust_id\", \"first_name\", \"last_name\", \"age\", \"gender\", \"birthday\")\n",
    ")\n",
    "\n",
    "df_narrow_transform.write.format(\"noop\").mode(\"overwrite\").save(\"E:/PySpark/data/test/df_narrow_transform.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------+----+------+---------+\n",
      "|cust_id   |first_name|last_name|age |gender|birthday |\n",
      "+----------+----------+---------+----+------+---------+\n",
      "|C007YEYTX9|Aaron     |Abbott   |39.0|Female|7/13/1991|\n",
      "|C08XAQUY73|Aaron     |Lambert  |59.0|Female|11/5/1966|\n",
      "|C094P1VXF9|Aaron     |Lindsey  |29.0|Male  |9/21/1990|\n",
      "|C097SHE1EF|Aaron     |Lopez    |27.0|Female|4/18/2001|\n",
      "|C0DTC6436T|Aaron     |Schwartz |57.0|Female|7/9/1962 |\n",
      "|C0R42FPHRH|Abbie     |Reyes    |68.0|Male  |10/8/1995|\n",
      "|C0RZV4BH7T|Abbie     |Stevenson|41.0|Male  |2/10/1971|\n",
      "+----------+----------+---------+----+------+---------+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_narrow_transform.show(7, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer_gt_50 = (\n",
    "    df_customers\n",
    "    .filter(F.col(\"age\").cast(\"int\") > 50)\n",
    ")\n",
    "df_customer_gt_50.write.format(\"noop\").mode(\"overwrite\").save(\"../data/test/df_customer_gt_50.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wide Transformations\n",
    "1. Joins\n",
    "   - Sort Merge Join\n",
    "   - Broadcast Join\n",
    "2. GroupBy\n",
    "   - `count`\n",
    "   - `countDistinct`\n",
    "   - `sum`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort Merge Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined = (\n",
    "    df_transactions.join(\n",
    "        df_customers,\n",
    "        how=\"inner\",\n",
    "        on=\"cust_id\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined.write.format(\"noop\").mode(\"overwrite\").save(\"E:/PySpark/data/test/df_joined.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcast Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 10485760)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_broadcast_joined = (\n",
    "    df_transactions.join(\n",
    "        F.broadcast(df_customers),\n",
    "        how=\"inner\",\n",
    "        on=\"cust_id\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_broadcast_joined.write.format(\"noop\").mode(\"overwrite\").save(\"E:/PySpark/data/test/df_broadcast_joined.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GroupBy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupBy Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city_counts = (\n",
    "    df_transactions\n",
    "    .groupBy(\"city\")\n",
    "    .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city_counts.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_txn_amt_city = (\n",
    "    df_transactions\n",
    "    .groupBy(\"city\")\n",
    "    .agg(F.sum(\"amt\").alias(\"txn_amt\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_txn_amt_city.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupBy Count Distinct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_txn_per_city = (\n",
    "    df_transactions\n",
    "    .groupBy(\"city\")\n",
    "    .agg(F.countDistinct(\"txn_id\").alias(\"txn_count\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_txn_per_city.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data-skew-explanation",
   "notebookOrigID": 4018749166498458,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
