	
  PySpark & Azure Databricks 
  ----------------------------
   Databricks Basics
	- Databricks Community basic features
	- Notebook basics & Magic commands
	- Databricks Utilities
   PySpark Essentials
        - Spark fundamentals, architecture
	- Spark SQL basics
	- Structured Streaming basics
	- Spark performance & tuning basics
   Azure Databricks 
    	- Azure Databricks Architecture
    	- Workspaces & Storage Accounts
    	- Access Patterns - Access-keys, SAS, Service principal
   	- Mount ADLS-Gen2 containers on Databricks
   Databricks Lakehouse Platform 
	â€“ Delta Lake using SQL & Python
	- Delta Lake Features
   ELT with Spark SQL
	- Querying from files
	- Writing to tables
	- HOFs & UDFs
   Incremental Data Processing 
	- COPY INTO command
	- Structured Streaming
	- AutoLoader
	- Medallion Pattern   
   Data Governance
	- Unity Catalog
   Production pipelines	
	- Delta Live Tables
	- Job Workflows


  Materials
  ---------
   - PDF presentations
   - Code Modules (PySpark)
   - Databricks Notebooks (DBC files)
   - Class notes (PySpark)
   - Github: 
	https://github.com/ykanakaraju/pyspark
	https://github.com/ykanakaraju/databricksazure

  Getting started with Databricks
  -------------------------------   
   ** Databricks Community Edition (free edition)
 		
	Signup: https://www.databricks.com/try-databricks
		Click on "Try Databricks" button (top-right corner)
		Toward the bottom you see:		  
			Looking for Databricks Community Edition? Click here
		-> Click on this link
		-> Provide a valid email
		-> Key in the validation code sent to your email.		

	Login: https://community.cloud.databricks.com/login.html

	Downloading a file from Databricks
	----------------------------------
		/FileStore/<FILEPATH>
		https://community.cloud.databricks.com/files/<FILEPATH>?o=1072576993312365

		Example:
		dbfs:/FileStore/output/wc/part-00000
		https://community.cloud.databricks.com/files/output/wc/part-00000?o=1072576993312365

		dbfs:/FileStore/output/wordcount1/part-00000
		https://community.cloud.databricks.com/files/output/wordcount1/part-00000?o=1072576993312365


 	Enabling DBFS File browser
	--------------------------
	<your account (top-right)> -> Settings -> Advanced -> Other -> DBFS File Browser (enable it)
								    -> & refresh the browser page

  Databricks Basics
  -----------------	
	1. Compute - Spin up cluster resources		
		1. All purpose clusters - continuously running, persistent cluster
		2. Job clusters - spinned up to execute a job and then terminated	

	2. Catalog
		-> All data is stored in Catalog
		-> All local files are store in /FileStore directory
		-> Default hive warehouse directory : /user/hive/warehouse

	3. Workspace
		-> All the notebooks are managed in the workspace


  Databricks 'dbutils'
  --------------------

	'help' command
	------------
	dbutils.help()
	dbutils.fs.help()
	dbutils.fs.help('ls')


	'ls' command
	------------
	dbutils.fs.ls("/")
	dbutils.fs.ls("/FileStore")
	dbutils.fs.ls("/FileStore/testdata/csv")


	'mkdirs' command
	----------------
	dbutils.fs.mkdirs("/FileStore/testdata2")
	dbutils.fs.ls("/FileStore")


	'cp' command to copy files between DBFS directories
	---------------------------------------------------
	dbutils.fs.cp("/FileStore/testdata/csv/2011_summary.csv", "/FileStore/testdata2")
	dbutils.fs.cp("/FileStore/testdata/csv/2012_summary.csv", "/FileStore/testdata2")
	dbutils.fs.cp("/FileStore/testdata/csv/2013_summary.csv", "/FileStore/testdata2")

	dbutils.fs.ls("/FileStore/testdata/csv")


	'mv' command for moving or renaming files
	------------------------------------------
	dbutils.fs.help('mv')

	dbutils.fs.mv("/FileStore/testdata/csv", "/FileStore/testdata3", recurse=True)
	dbutils.fs.ls('/FileStore/testdata3')

	dbutils.fs.mv('/FileStore/testdata3/2011_summary.csv', '/FileStore/testdata3/2011_summary_renamed.csv')


	'rm' command to remove files & directories from DBFS
	-----------------------------------------------------
	dbutils.fs.rm('/FileStore/testdata3/2011_summary_renamed.csv')
	dbutils.fs.rm('/FileStore/testdata3/', recurse=True)


 ====================
      Spark
 ====================

	Spark is an open-source in-memory distributed computing framework.

	-> Spark is written in Scala

	-> Spark is a ployglot  
		-> Spark applications can be written in multiple languages
		-> Supports Python, R, Scala, Java, SQL

	-> Spark can run on multiple cluster managers
		-> Supports local, spark standalone, YARN, Mesos, Kubernetes


    Spark is a unified framework.
    -----------------------------
	
	Spark provides a consistent set of APIs for performing different analytics workloads
        using the same execution engine and some well defined data abstractions and operations.

   	Batch Analytics of unstructured data	-> Spark Core API (low-level api)
	Batch Analytics of structured data 	-> Spark SQL
	Streaming Analytics (real time)		-> Spark Streaming (DStreams API), Structured Streaming
	Predictive analytics (ML)		-> Spark MLLib  (mllib & ml)
	Graph parallel computations  		-> Spark GraphX

  
   Spark Architecture
   ------------------

	1. Cluster Manager (CM)
		-> Applications are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes

	2. Driver
		-> Master process
		-> Runs the 'SparkContext' object
		-> Manages the user code and sends the tasks to the executors as per the code. 
		
	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> Receives the tasks from the Driver
		-> All tasks run the same code but on different partitions of the data
		-> The status of tasks are reported to the driver. 	



  Setting up PySpark on Windows machine
  -------------------------------------

	1. Install Java 8 or up

		java -version
		
		https://www.oracle.com/in/java/technologies/downloads/#jdk24-windows

	2. Add JAVA_HOME environment variable

		Go to 'Edit system environemt variables' windows
		Add JAVA_HOME env. variable with the PATH were Java is installed.

	3. Download and extract Spark binaries.

		URL: https://spark.apache.org/downloads.html
		Choose the version
		Download spark: <click on this link>
		Go to the mirror site and download


		-> Extract the downloaded file in a suitable folder
		(tar -xvf <path-of-tgz file>)

	4. Add SPARK_HOME environment variable

		Go to 'Edit system environemt variables' windows
		Add SPARK_HOME env. variable with the PATH were Spark is extracted.

	5. Setup Hadoop winutils for windows 

		URL: https://github.com/cdarlint/winutils
		Download the repo
		Extract it and copy the folder to your Spark path. 

	6. Add HADOOP_HOME environment variable

		Go to 'Edit system environemt variables' windows
		Add HADOOP_HOME env. variable with the PATH were hadoop is extracted.

	7. Add the "bin" folders of the above to the PATH environment variable

		%JAVA_HOME%\bin
		%SPARK_HOME%\bin
		%HADOOP_HOME%\bin

	8. Open a command terminal and type spark-shell

		C:> pyspark

	9. Download and install Python (3 or above)

	10. Pip install find-spark library for python

		pip install findspark


   RDD (Resilient Distributed Dataset)
   -----------------------------------

	-> RDD is the fundamental data abstraction of Spark

	-> RDD is a collection of distributed in-memory partitions.
	    -> Each partition is a collection of objects of some type.

	-> RDDs are immutable

	-> RDDs are lazily evaluated
	   -> Transformations does not cause execution.
	   -> Action commands trigger execution.


  Creating RDDs
  -------------
	
	Three ways:

	1. Creating an RDD from external data file
		rdd1 = sc.textFile(<dataPath>, 4)

	2. Creating an RDD from programmatic data
		rdd1 = sc.parallelize([2,1,3,2,4,3,5,4,6,7,5,6,7,6,8,8,9,0], 2)

	3. By applying transformations on existing RDDs
		rdd2 = rdd1.map(lambda x: x*10)


  RDD Operations
  --------------

    Two types of operations

	1. Transformations
		-> Transformations return RDDs
		-> Transformations does not cause execution of RDDs
		-> Cause lineage DAGs to be created

	2. Actions
		-> Triggers execution of RDDs
		-> Produces output by sending a Job to the cluster


  RDD Lineage DAG
  ---------------
    Driver maintains a Lineage DAG for every RDD.
    Lineage DAG is a heirarchy of dependencies of RDDs all the way starting from the very first RDD.	
    Lineage DAG is a logical plan on how to create the RDD.


	rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)
	DAG of rddFile:  (4) rddFile -> sc.textFile on E:\\Spark\\wordcount.txt

	rddWords = rddFile.flatMap(lambda x: x.split())
	DAG of rddWords: (4) rddWords -> rddFile.flatMap -> sc.textFile on E:\\Spark\\wordcount.txt

	rddPairs = rddWords.map(lambda x: (x, 1))	
	DAG of rddPairs: (4) rddPairs -> rddWords.map -> rddFile.flatMap -> sc.textFile 
	
	rddWc = rddPairs.reduceByKey(lambda x, y: x + y)
	DAG of rddWc: (4) rddWc -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> sc.textFile 



  Types of Transformations
  ------------------------

	Two types:

	1. Narrow Transformations
		-> Narrow transformations are those, where the computation of each partition depends ONLY
		  on its input partition.
		-> There is no shuffling of data.
		-> Simple and efficient

	2. Wide Transformations
		-> In wide transformations, the computation of a single partition depends on many
		  partitions of its input RDD.
		-> Data shuffle across partitions will happen.
		-> Complex and expensive


  RDD Transformations
  -------------------

  1. map		P: U -> V
			Object to object transformation
			Input RDD: N objects, Output RDD: N objects					

  		rdd2 = rddFile.map(lambda x : x.split())


   2. filter		P: U -> Boolean
			Filters the objects based on the function
			Input RDD: N objects, Output RDD: <= N objects	

		rddFile.filter(lambda x: len(x.split()) > 8 ).collect()


   3. glom		P: None
			Return one list object per partition with all the objects of the partition
			Input RDD: N objects, Output RDD: number of objects = number of partitions

		rdd1	    rdd2 = rdd1.glom()

		P0: 3,4,2,4,6  -> glom -> P0: [3,4,2,4,6]
		P1: 4,6,7,8,6  -> glom -> P1: [4,6,7,8,6] 
		P2: 5,0,7,9,1  -> glom -> P2: [5,0,7,9,1] 



   4. flatMap		P: U -> Iterable[V]
			flattens the iterables generated by the function
 			Input RDD: N objects, Output RDD: >= N objects	

		rddWords = rddFile.flatMap(lambda x: x.split())


      Types of RDDs
      -------------
	Generic RDDs: 	RDD[U]            
	Pair RDD: 	RDD[(K, V)]

 
   5. mapValues		P: U => V
			Applied only on Pair RDDs
			Applies the function only to the 'value' part of the key-value pairs

		rddPairs.mapValues(lambda x: x*10).collect()
		-> In the above fn, x represents only the 'value' part of key-value pairs



  6. sortBy		P: U -> V, Optional: ascending (True/False), numPartitions
			Sorts the RDDs based on the function output.

		rdd1.sortBy(lambda x: x%3).glom().collect()
		rdd1.sortBy(lambda x: x%3, False).glom().collect()
		rdd1.sortBy(lambda x: x%3, True, 2).glom().collect()
	 

  7. groupBy		P: U -> V, Optional: numPartitions

			Returns a Pair-RDD where:
			    key: Each unique value of the function output
		            value: ResultIterable. Grouped objects of the RDD that produced the key 





  Spark DAG Scheduler
  -------------------

	==> Continue from here on 9th June



























