

  Lab 3: Working with databases and table in Databricks
  ----------------------------------------------
  
  1. Upload the following directory to DBFS
	
		directory: retail_db   (under datasets folder)
  
  2. Create a notebook and select "SQL" as the language and attcah it to a running cluster
  
  3. Perform various SQL operations.
  
	3.1 Basic Database Operations
  
		CREATE DATABASE retaildb;
		SHOW DATABASES;
		
		USE retaildb;
		SELECT current_database();
		
		DESC DATABASE retaildb;
		
	3.2 Create managed table		

		CREATE TABLE retaildb.categories (
		  category_id INT,
		  category_department_id INT,
		  category_name STRING
		)
		ROW FORMAT DELIMITED
		FIELDS TERMINATED BY ',';
		
		SHOW TABLES;
		
	3.3 Load data file into the table and insert rows into the table
		
		LOAD DATA LOCAL INPATH 'dbfs:/FileStore/retail_db/categories/part_00000' 
		OVERWRITE INTO TABLE retaildb.categories;

		DESC FORMATTED retaildb.categories;
		
		INSERT INTO retaildb.categories
		category_id, category_department_id, category_name)
		VALUES(101, 2, 'Cricket'), (102, 6, 'Badminton Racquet');
		
		SELECT * FROM retaildb.categories ORDER BY category_id DESC;
		
	3.4 Update and delete operations (not supported)
		
		UPDATE retaildb.categories SET category_name = 'Chess' WHERE category_id = 101;
		-> Update is not supported in Hive tables. 
		-> Supported only in Delta tables.
		
		DELETE FROM retaildb.categories WHERE category_id = 101;
		-> Delete is not supported in Hive tables. 
				
	3.5 External tables
	
		CREATE EXTERNAL TABLE retaildb.categories_external (
		  category_id INT,
		  category_department_id INT,
		  category_name STRING
		) 
		ROW FORMAT DELIMITED 
		FIELDS TERMINATED BY ',' 
		LOCATION 'dbfs:/FileStore/retail_db/categories';
		
		
		LOAD DATA LOCAL INPATH 'dbfs:/FileStore/retail_db/categories/part_00000' 
		OVERWRITE INTO TABLE retaildb.categories;
		
		SELECT * FROM retaildb.categories_external;
		
	3.6 Working with supported data formats (like Parquet, ORC)
	
		CREATE TABLE retaildb.categories (
		  category_id INT,
		  category_department_id INT,
		  category_name STRING
		) STORED AS PARQUET;
		
		INSERT INTO retaildb.categories_parquet
		SELECT * FROM retaildb.categories;
		
		%fs 
		ls /user/hive/warehouse/retaildb.db/categories_parquet
		
		%python
		spark.read.format('parquet').load('/user/hive/warehouse/retaildb.db/categories_parquet').show()
		
  4. Work with 'Local Tables' or 'LocalTempViews'
  
		%python

		filePath = "dbfs:/FileStore/retail_db/customers/part_00000"
		mySchema = "id INT, fname STRING, lname STRING, c3 STRING, c4 STRING, address STRING, city STRING, state STRING, zip INT"

		customersDf = spark.read.csv(filePath, schema=mySchema) \
					.drop("c3", "c4")
		display(customersDf)

		customersDf.createOrReplaceTempView("customers")

	
  5. Work with partitioned tables
  
		%python

		#display(customersDf)

		customersDf \
			.write \
			.partitionBy("state") \
			.format("parquet") \
			.saveAsTable("customers_partitioned")

		# actual file will not have partitioned column data.
		path = "dbfs:/user/hive/warehouse/retaildb.db/customers_partitioned/state=CA/*.parquet"

		spark \
			.read \
			.format("parquet") \
			.load(path) \
			.show()
			
		# If you query from the root-level, you get all data
		path2 = "dbfs:/user/hive/warehouse/retaildb.db/customers_partitioned/"

		spark \
			.read \
			.format("parquet") \
			.load(path2) \
			.show()
	
				
		
  
		

NOTES:
1. When you create a table using notebook, by default, the table is created using 'Delta' format. 
2. Delta format is the default in Databricks, not Parquet.
3. LOAD DATA command is not supported on delta tables.
4. UPDATE destination only supports Delta sources.



		




