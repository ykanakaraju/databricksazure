{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12fa052b-65ab-42fe-92e9-866a91ab0479",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_path = \"E:\\\\Spark\\\\spark-3.2.0-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(spark_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e02171b5-8b8d-4b81-8826-2fb3ee1a8a24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Spark\\spark-3.2.0-bin-hadoop2.7\\python\\pyspark\\context.py:238: FutureWarning: Python 3.6 support is deprecated in Spark 3.2.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-3KI6KTQ:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x133fd5f0860>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_file = \"E:/PySpark/data/transactions.parquet\"\n",
    "df_transactions = spark.read.parquet(transactions_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+-----+------------+\n",
      "|cust_id   |start_date|end_date  |txn_id         |date      |year|month|day|expense_type |amt  |city        |\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+-----+------------+\n",
      "|CCWG6X0ANH|2010-07-01|2019-10-01|TXCC7I6BMRX3XIT|2015-06-27|2015|6    |27 |Entertainment|35.18|denver      |\n",
      "|CCWG6X0ANH|2010-07-01|2019-10-01|TGW6THZKM8C6SUF|2016-08-02|2016|8    |2  |Entertainment|58.15|new_york    |\n",
      "|CCWG6X0ANH|2010-07-01|2019-10-01|TOLMMCXS90H2K31|2014-09-17|2014|9    |17 |Groceries    |43.92|boston      |\n",
      "|CCWG6X0ANH|2010-07-01|2019-10-01|TXLH0P8JII96Q7U|2012-10-05|2012|10   |5  |Entertainment|35.2 |philadelphia|\n",
      "|CCWG6X0ANH|2010-07-01|2019-10-01|TP6RDK6P58M2RO4|2015-10-21|2015|10   |21 |Entertainment|47.44|chicago     |\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+-----+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transactions.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_file = \"E:/PySpark/data/customers.parquet\"\n",
    "df_customers = spark.read.parquet(customers_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+---+------+----------+-----+-----------+\n",
      "|cust_id   |name         |age|gender|birthday  |zip  |city       |\n",
      "+----------+-------------+---+------+----------+-----+-----------+\n",
      "|C007YEYTX9|Aaron Abbott |34 |Female|7/13/1991 |97823|boston     |\n",
      "|C00B971T1J|Aaron Austin |37 |Female|12/16/2004|30332|chicago    |\n",
      "|C00WRSJF1Q|Aaron Barnes |29 |Female|3/11/1977 |23451|denver     |\n",
      "|C01AZWQMF3|Aaron Barrett|31 |Male  |7/9/1998  |46613|los_angeles|\n",
      "|C01BKUFRHA|Aaron Becker |54 |Male  |11/24/1979|40284|san_diego  |\n",
      "+----------+-------------+---+------+----------+-----+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_customers.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark's Query Plans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Narrow Transformations\n",
    "- `filter` rows where `city='boston'`\n",
    "- `add` a new column: adding `first_name` and `last_name`\n",
    "- `alter` an exisitng column: adding 5 to `age` column\n",
    "- `select` relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------+----+------+---------+\n",
      "|cust_id   |first_name|last_name|age |gender|birthday |\n",
      "+----------+----------+---------+----+------+---------+\n",
      "|C007YEYTX9|Aaron     |Abbott   |39.0|Female|7/13/1991|\n",
      "|C08XAQUY73|Aaron     |Lambert  |59.0|Female|11/5/1966|\n",
      "|C094P1VXF9|Aaron     |Lindsey  |29.0|Male  |9/21/1990|\n",
      "|C097SHE1EF|Aaron     |Lopez    |27.0|Female|4/18/2001|\n",
      "|C0DTC6436T|Aaron     |Schwartz |57.0|Female|7/9/1962 |\n",
      "+----------+----------+---------+----+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_narrow_transform = (\n",
    "    df_customers\n",
    "    .filter(F.col(\"city\") == \"boston\")\n",
    "    .withColumn(\"first_name\", F.split(\"name\", \" \").getItem(0))\n",
    "    .withColumn(\"last_name\", F.split(\"name\", \" \").getItem(1))\n",
    "    .withColumn(\"age\", F.col(\"age\") + F.lit(5))\n",
    "    .select(\"cust_id\", \"first_name\", \"last_name\", \"age\", \"gender\", \"birthday\")\n",
    ")\n",
    "\n",
    "df_narrow_transform.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project ['cust_id, 'first_name, 'last_name, 'age, 'gender, 'birthday]\n",
      "+- Project [cust_id#67, name#68, (cast(age#69 as double) + cast(5 as double)) AS age#129, gender#70, birthday#71, zip#72, city#73, first_name#110, last_name#119]\n",
      "   +- Project [cust_id#67, name#68, age#69, gender#70, birthday#71, zip#72, city#73, first_name#110, split(name#68,  , -1)[1] AS last_name#119]\n",
      "      +- Project [cust_id#67, name#68, age#69, gender#70, birthday#71, zip#72, city#73, split(name#68,  , -1)[0] AS first_name#110]\n",
      "         +- Filter (city#73 = boston)\n",
      "            +- Relation [cust_id#67,name#68,age#69,gender#70,birthday#71,zip#72,city#73] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "cust_id: string, first_name: string, last_name: string, age: double, gender: string, birthday: string\n",
      "Project [cust_id#67, first_name#110, last_name#119, age#129, gender#70, birthday#71]\n",
      "+- Project [cust_id#67, name#68, (cast(age#69 as double) + cast(5 as double)) AS age#129, gender#70, birthday#71, zip#72, city#73, first_name#110, last_name#119]\n",
      "   +- Project [cust_id#67, name#68, age#69, gender#70, birthday#71, zip#72, city#73, first_name#110, split(name#68,  , -1)[1] AS last_name#119]\n",
      "      +- Project [cust_id#67, name#68, age#69, gender#70, birthday#71, zip#72, city#73, split(name#68,  , -1)[0] AS first_name#110]\n",
      "         +- Filter (city#73 = boston)\n",
      "            +- Relation [cust_id#67,name#68,age#69,gender#70,birthday#71,zip#72,city#73] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [cust_id#67, split(name#68,  , -1)[0] AS first_name#110, split(name#68,  , -1)[1] AS last_name#119, (cast(age#69 as double) + 5.0) AS age#129, gender#70, birthday#71]\n",
      "+- Filter (isnotnull(city#73) AND (city#73 = boston))\n",
      "   +- Relation [cust_id#67,name#68,age#69,gender#70,birthday#71,zip#72,city#73] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [cust_id#67, split(name#68,  , -1)[0] AS first_name#110, split(name#68,  , -1)[1] AS last_name#119, (cast(age#69 as double) + 5.0) AS age#129, gender#70, birthday#71]\n",
      "+- *(1) Filter (isnotnull(city#73) AND (city#73 = boston))\n",
      "   +- *(1) ColumnarToRow\n",
      "      +- FileScan parquet [cust_id#67,name#68,age#69,gender#70,birthday#71,city#73] Batched: true, DataFilters: [isnotnull(city#73), (city#73 = boston)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/E:/PySpark/data/customers.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(city), EqualTo(city,boston)], ReadSchema: struct<cust_id:string,name:string,age:string,gender:string,birthday:string,city:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_narrow_transform.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wide Transformations\n",
    "1. Repartition\n",
    "2. Coalesce\n",
    "3. Joins\n",
    "4. GroupBy\n",
    "   - `count`\n",
    "   - `countDistinct`\n",
    "   - `sum`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transactions.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Repartition 24, true\n",
      "+- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "cust_id: string, start_date: string, end_date: string, txn_id: string, date: string, year: string, month: string, day: string, expense_type: string, amt: string, city: string\n",
      "Repartition 24, true\n",
      "+- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Repartition 24, true\n",
      "+- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Exchange RoundRobinPartitioning(24), REPARTITION_BY_NUM, [id=#72]\n",
      "   +- FileScan parquet [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/E:/PySpark/data/transactions.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<cust_id:string,start_date:string,end_date:string,txn_id:string,date:string,year:string,mon...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transactions.repartition(24).explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------------+------+------------+\n",
      "|   cust_id|start_date|  end_date|         txn_id|      date|year|month|day|       expense_type|   amt|        city|\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------------+------+------------+\n",
      "|C0YDPQWPBJ|2010-04-01|2019-09-01|TVXUVPGJMNDVJNT|2016-10-09|2016|   10|  9|Bills and Utilities|260.43|philadelphia|\n",
      "|CGC6V1LCDF|2010-07-01|2020-03-01|TIY34BZINOGEBR1|2015-06-27|2015|    6| 27|      Entertainment| 16.25|      boston|\n",
      "|C0YDPQWPBJ|2011-06-01|2020-07-01|TO4XD5EU6MFQ7XU|2017-06-07|2017|    6|  7|      Entertainment|  9.02|     seattle|\n",
      "|C0YDPQWPBJ|2011-12-01|      null|T8J5TXH2ZNQZ1N8|2016-02-05|2016|    2|  5|       Motor/Travel| 30.07|philadelphia|\n",
      "|C0YDPQWPBJ|2011-09-01|2020-04-01|TFA4WZVS391HZ5R|2017-07-26|2017|    7| 26|      Entertainment| 17.64|    new_york|\n",
      "|C0YDPQWPBJ|2011-06-01|2020-07-01|TMQIXF7Q05THK6Z|2019-06-05|2019|    6|  5|       Motor/Travel| 64.71|     seattle|\n",
      "|C0YDPQWPBJ|2012-04-01|      null|T0G7ZNIRM7BR53K|2016-02-24|2016|    2| 24|          Groceries| 15.78|      denver|\n",
      "|CA1OGDCSV2|2010-08-01|2019-06-01|T3V7I2H2HZ2F0RW|2012-02-10|2012|    2| 10|      Entertainment|  8.21|      boston|\n",
      "|C0YDPQWPBJ|2010-04-01|2019-09-01|T57HA9EHU568B8K|2019-06-24|2019|    6| 24|      Entertainment| 41.55|      boston|\n",
      "|CUUBGNEMR0|2010-05-01|2018-10-01|TWENJIYXQN9RL7B|2010-06-10|2010|    6| 10|      Entertainment| 22.69| los_angeles|\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------------+------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transactions.repartition(24).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Repartition 5, false\n",
      "+- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "cust_id: string, start_date: string, end_date: string, txn_id: string, date: string, year: string, month: string, day: string, expense_type: string, amt: string, city: string\n",
      "Repartition 5, false\n",
      "+- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Repartition 5, false\n",
      "+- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "Coalesce 5\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/E:/PySpark/data/transactions.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<cust_id:string,start_date:string,end_date:string,txn_id:string,date:string,year:string,mon...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transactions.coalesce(5).explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+------------+\n",
      "|   cust_id|start_date|  end_date|         txn_id|      date|year|month|day| expense_type|   amt|        city|\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+------------+\n",
      "|CCWG6X0ANH|2010-07-01|2019-10-01|TXCC7I6BMRX3XIT|2015-06-27|2015|    6| 27|Entertainment| 35.18|      denver|\n",
      "|CCWG6X0ANH|2010-07-01|2019-10-01|TGW6THZKM8C6SUF|2016-08-02|2016|    8|  2|Entertainment| 58.15|    new_york|\n",
      "|CCWG6X0ANH|2010-07-01|2019-10-01|TOLMMCXS90H2K31|2014-09-17|2014|    9| 17|    Groceries| 43.92|      boston|\n",
      "|CCWG6X0ANH|2010-07-01|2019-10-01|TXLH0P8JII96Q7U|2012-10-05|2012|   10|  5|Entertainment|  35.2|philadelphia|\n",
      "|CCWG6X0ANH|2010-07-01|2019-10-01|TP6RDK6P58M2RO4|2015-10-21|2015|   10| 21|Entertainment| 47.44|     chicago|\n",
      "|CCWG6X0ANH|2010-07-01|2019-10-01|TCTF14CSG65AEFW|2018-07-18|2018|    7| 18|Entertainment| 29.67| los_angeles|\n",
      "|CCWG6X0ANH|2010-07-01|2019-10-01|TY7ZF4RZSAHH4OB|2014-04-23|2014|    4| 23| Motor/Travel|129.74|      denver|\n",
      "|CCWG6X0ANH|2010-07-01|2019-10-01|TF5QUF92DVVE5GO|2017-01-08|2017|    1|  8|          Tax|312.66|    new_york|\n",
      "|CCWG6X0ANH|2010-07-01|2019-10-01|TMTEMY3EEZXKSW2|2015-01-23|2015|    1| 23|Entertainment| 40.98|     seattle|\n",
      "|CCWG6X0ANH|2010-07-01|2019-10-01|TXQ5S9DN55EJ954|2019-06-18|2019|    6| 18|Entertainment| 38.59| los_angeles|\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transactions.coalesce(5).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why doesn't `.coalesce()` explicitly show the partitioning scheme?\n",
    "\n",
    "`.coalesce` doesn't show the partitioning scheme e.g. `RoundRobinPartitioning` because: \n",
    "- The operation only minimizes data movement by merging into fewer partitions, it doesn't do any shuffling.\n",
    "- Because no shuffling is done, the partitioning scheme remains the same as the original DataFrame and Spark doesn't include it explicitly in it's plan as the partitioning scheme is unaffected by `.coalesce`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Joins (Shuffle Sort Merge Join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-1'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined = (\n",
    "    df_transactions.join(\n",
    "        df_customers,\n",
    "        how=\"inner\",\n",
    "        on=\"cust_id\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(Inner,Buffer(cust_id))\n",
      ":- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "+- Relation [cust_id#67,name#68,age#69,gender#70,birthday#71,zip#72,city#73] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "cust_id: string, start_date: string, end_date: string, txn_id: string, date: string, year: string, month: string, day: string, expense_type: string, amt: string, city: string, name: string, age: string, gender: string, birthday: string, zip: string, city: string\n",
      "Project [cust_id#0, start_date#1, end_date#2, txn_id#3, date#4, year#5, month#6, day#7, expense_type#8, amt#9, city#10, name#68, age#69, gender#70, birthday#71, zip#72, city#73]\n",
      "+- Join Inner, (cust_id#0 = cust_id#67)\n",
      "   :- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "   +- Relation [cust_id#67,name#68,age#69,gender#70,birthday#71,zip#72,city#73] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [cust_id#0, start_date#1, end_date#2, txn_id#3, date#4, year#5, month#6, day#7, expense_type#8, amt#9, city#10, name#68, age#69, gender#70, birthday#71, zip#72, city#73]\n",
      "+- Join Inner, (cust_id#0 = cust_id#67)\n",
      "   :- Filter isnotnull(cust_id#0)\n",
      "   :  +- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "   +- Filter isnotnull(cust_id#67)\n",
      "      +- Relation [cust_id#67,name#68,age#69,gender#70,birthday#71,zip#72,city#73] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [cust_id#0, start_date#1, end_date#2, txn_id#3, date#4, year#5, month#6, day#7, expense_type#8, amt#9, city#10, name#68, age#69, gender#70, birthday#71, zip#72, city#73]\n",
      "   +- SortMergeJoin [cust_id#0], [cust_id#67], Inner\n",
      "      :- Sort [cust_id#0 ASC NULLS FIRST], false, 0\n",
      "      :  +- Exchange hashpartitioning(cust_id#0, 200), ENSURE_REQUIREMENTS, [id=#160]\n",
      "      :     +- Filter isnotnull(cust_id#0)\n",
      "      :        +- FileScan parquet [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] Batched: true, DataFilters: [isnotnull(cust_id#0)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/E:/PySpark/data/transactions.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(cust_id)], ReadSchema: struct<cust_id:string,start_date:string,end_date:string,txn_id:string,date:string,year:string,mon...\n",
      "      +- Sort [cust_id#67 ASC NULLS FIRST], false, 0\n",
      "         +- Exchange hashpartitioning(cust_id#67, 200), ENSURE_REQUIREMENTS, [id=#161]\n",
      "            +- Filter isnotnull(cust_id#67)\n",
      "               +- FileScan parquet [cust_id#67,name#68,age#69,gender#70,birthday#71,zip#72,city#73] Batched: true, DataFilters: [isnotnull(cust_id#67)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/E:/PySpark/data/customers.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(cust_id)], ReadSchema: struct<cust_id:string,name:string,age:string,gender:string,birthday:string,zip:string,city:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-------------+----------+---+------+---------+-----+--------+\n",
      "|   cust_id|start_date|  end_date|         txn_id|      date|year|month|day| expense_type|   amt|         city|      name|age|gender| birthday|  zip|    city|\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-------------+----------+---+------+---------+-----+--------+\n",
      "|C0Y61QV84S|2011-01-01|2020-03-01|TNQ5HWKS4IJ0ADX|2011-07-04|2011|    7|  4|Entertainment| 11.16|san_francisco|Ada Keller| 26|Female|6/25/1961|77925|portland|\n",
      "|C0Y61QV84S|2011-01-01|2020-03-01|T500LLXZ7S5SU5J|2019-07-01|2019|    7|  1|          Tax|125.19|     new_york|Ada Keller| 26|Female|6/25/1961|77925|portland|\n",
      "|C0Y61QV84S|2011-01-01|2020-03-01|TNS0XYGHCDB68KM|2017-05-04|2017|    5|  4|Entertainment| 23.41|  los_angeles|Ada Keller| 26|Female|6/25/1961|77925|portland|\n",
      "|C0Y61QV84S|2011-01-01|2020-03-01|TLZVV9F60FFIRB1|2019-10-12|2019|   10| 12| Motor/Travel| 52.15|      chicago|Ada Keller| 26|Female|6/25/1961|77925|portland|\n",
      "|C0Y61QV84S|2011-01-01|2020-03-01|TM1QRJP0O9AGT2Q|2015-01-06|2015|    1|  6|Entertainment| 14.02|san_francisco|Ada Keller| 26|Female|6/25/1961|77925|portland|\n",
      "|C0Y61QV84S|2011-01-01|2020-03-01|T8GHSW54WAAA2IC|2011-07-01|2011|    7|  1|    Groceries| 10.59|san_francisco|Ada Keller| 26|Female|6/25/1961|77925|portland|\n",
      "|C0Y61QV84S|2011-01-01|2020-03-01|TOXU8VVSM2N15GQ|2016-10-23|2016|   10| 23| Motor/Travel| 33.28| philadelphia|Ada Keller| 26|Female|6/25/1961|77925|portland|\n",
      "|C0Y61QV84S|2011-01-01|2020-03-01|TIJEOXNW1G61V3U|2014-02-27|2014|    2| 27|Entertainment|  13.6|     portland|Ada Keller| 26|Female|6/25/1961|77925|portland|\n",
      "|C0Y61QV84S|2011-01-01|2020-03-01|T0608KTTV3YDEK2|2011-04-18|2011|    4| 18|Entertainment|  7.34|      chicago|Ada Keller| 26|Female|6/25/1961|77925|portland|\n",
      "|C0Y61QV84S|2011-01-01|2020-03-01|TR95JA45WVZEFQY|2018-07-04|2018|    7|  4|Entertainment| 17.26|       denver|Ada Keller| 26|Female|6/25/1961|77925|portland|\n",
      "|C0Y61QV84S|2011-01-01|2020-03-01|TJCROCBAY1U5G3I|2019-03-26|2019|    3| 26| Motor/Travel| 41.53|       boston|Ada Keller| 26|Female|6/25/1961|77925|portland|\n",
      "|C0Y61QV84S|2011-01-01|2020-03-01|TPV097J5TXLYLBU|2012-12-17|2012|   12| 17|    Groceries|  11.7|     portland|Ada Keller| 26|Female|6/25/1961|77925|portland|\n",
      "|C0Y61QV84S|2011-01-01|2020-03-01|TTQ9HJ2EU9FWK4B|2017-06-24|2017|    6| 24|Entertainment|  6.85|san_francisco|Ada Keller| 26|Female|6/25/1961|77925|portland|\n",
      "|C0Y61QV84S|2011-01-01|2020-03-01|TH9BRR5VRQI0RPB|2018-01-01|2018|    1|  1|          Tax|125.19|      chicago|Ada Keller| 26|Female|6/25/1961|77925|portland|\n",
      "|C0Y61QV84S|2011-01-01|2020-03-01|T50PKG9YUN9K3D5|2017-04-24|2017|    4| 24|    Groceries| 11.89|     new_york|Ada Keller| 26|Female|6/25/1961|77925|portland|\n",
      "|C0Y61QV84S|2011-01-01|2020-03-01|T7C584OKSN577PU|2014-07-16|2014|    7| 16|Entertainment| 12.57|      chicago|Ada Keller| 26|Female|6/25/1961|77925|portland|\n",
      "|C0Y61QV84S|2011-01-01|2020-03-01|TZLIW9EOX1UADF0|2017-06-15|2017|    6| 15|Entertainment| 11.28|       denver|Ada Keller| 26|Female|6/25/1961|77925|portland|\n",
      "|C0Y61QV84S|2011-01-01|2020-03-01|T8GMB23E642ACXT|2017-01-12|2017|    1| 12|Entertainment| 15.78|      chicago|Ada Keller| 26|Female|6/25/1961|77925|portland|\n",
      "|C0Y61QV84S|2011-01-01|2020-03-01|TIKQDJ0E3ZP5EWM|2011-03-11|2011|    3| 11| Motor/Travel| 13.57|      seattle|Ada Keller| 26|Female|6/25/1961|77925|portland|\n",
      "|C0Y61QV84S|2011-01-01|2020-03-01|TLJ67SZ110EWOOG|2015-06-04|2015|    6|  4|Entertainment| 17.28|  los_angeles|Ada Keller| 26|Female|6/25/1961|77925|portland|\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-------------+----------+---+------+---------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GroupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupBy Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city_counts = (\n",
    "    df_transactions\n",
    "    .groupBy(\"city\")\n",
    "    .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city_counts.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_txn_amt_city = (\n",
    "    df_transactions\n",
    "    .groupBy(\"city\")\n",
    "    .agg(F.sum(\"amt\").alias(\"txn_amt\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_txn_amt_city.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupBy - countDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_txn_per_city = (\n",
    "    df_transactions\n",
    "    .groupBy(\"cust_id\")\n",
    "    .agg(F.countDistinct(\"city\").alias(\"city_count\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_txn_per_city.show(5, False)\n",
    "df_txn_per_city.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Some Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is a filter step present despite predicate pushdown? \n",
    "\n",
    "This is largely due to the way `Spark's Catalyst Optimizer` works. Specifically, due to two separate stages of the query optimization process: Physical Planning and Logical Planning.\n",
    "\n",
    "- **Logical Planning**: Catalyst optimizer simplifies the unresolved logical plan (which represents the user's query) by applying various rule-based optimizations. This includes `predicate pushdown`, `projection pushdown` where filter conditions and column projections are moved as close to the data source as possible.\n",
    "\n",
    "- **Physical Planning** phase is where the logical plan is translated into one or more physical plans, which can actually be executed on the cluster. This includes operations like file `scans`, `filters`, `projections`, etc.\n",
    "\n",
    "In this case, during the logical planning phase, the predicate (`F.col(\"city\") == \"boston\"`) has been pushed down and will be applied during the scan of the Parquet file (`PushedFilters: [IsNotNull(city), EqualTo(city,boston)]`), thus improving performance.\n",
    "\n",
    "Now, during the physical planning phase, the same filter condition (`+- *(1) Filter (isnotnull(city#73) AND (city#73 = boston))`) is applied again to the data that's been loaded into memory. This is because of the following reasons:\n",
    "\n",
    "1. **Guaranteed Correctness:** It might seem **redundant**, but remember that not all data sources can handle pushed-down predicates, and not all predicates can be pushed down. Therefore, **even if a predicate is pushed down to the data source, Spark still includes the predicate in the physical plan** to cover cases where the data source might not have been able to fully apply the predicate. This is Spark's way of making sure the correct data is always returned, no matter the capabilities of the data source.\n",
    "\n",
    "2. **No Assumptions**: Spark's Catalyst optimizer doesn't make assumptions about the data source's ability to handle pushed-down predicates. The optimizer aims to generate plans that return correct results across a wide range of scenarios. Even if the filter is pushed down, Spark does not have the feedback from data source whether the pushdown was successful or not, so it includes the filter operation in the physical plan as well.\n",
    "\n",
    "It is more of a **fail-safe mechanism** to ensure data **integrity** and **correctness**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In what cases will predicate pushdown not work?\n",
    "\n",
    "2 Examples where **filter pushdown** will not work:\n",
    "\n",
    "1. **Complex Data Types**: Spark's Parquet data source does not push down filters that involve **complex types**, such as **arrays**, **maps**, and **structs**. This is because these complex data types can have complicated nested structures that the Parquet reader cannot easily filter on.\n",
    "\n",
    "Here's an example:\n",
    "\n",
    "```\n",
    "root\n",
    " |-- Name: string (nullable = true)\n",
    " |-- properties: map (nullable = true)\n",
    " |    |-- key: string\n",
    " |    |-- value: string (valueContainsNull = true)\n",
    "\n",
    "+----------+-----------------------------+\n",
    "|Name      |properties                   |\n",
    "+----------+-----------------------------+\n",
    "|Afaque    |[eye -> black, hair -> black]|\n",
    "|Naved     |[eye ->, hair -> brown]      |\n",
    "|Ali       |[eye -> black, hair -> red]  |\n",
    "|Amaan     |[eye -> grey, hair -> grey]  |\n",
    "|Omaira    |[eye -> , hair -> brown]     |\n",
    "+----------+-----------------------------+\n",
    "```\n",
    "\n",
    "```python\n",
    "df.filter(df.properties.getItem(\"eye\") == \"brown\").show()\n",
    "```\n",
    "\n",
    "```\n",
    "== Physical Plan ==\n",
    "*(1) Filter (metadata#123[key] = value)\n",
    "+- *(1) ColumnarToRow\n",
    "   +- FileScan parquet [id#122,metadata#123] Batched: true, DataFilters: [(metadata#123[key] = value)], Format: Parquet, ...\n",
    "```\n",
    "\n",
    "------------------------------------------------\n",
    "\n",
    "3. Unsupported Expressions: \n",
    "\n",
    "In Spark, `Parquet` data source does not support pushdown for filters involving a `.cast` operation. The reason for this behaviour is as follows:\n",
    "- `.cast` changes the datatype of the column, and the Parquet data source may not be able to perform the filter operation correctly on the cast data.\n",
    "\n",
    "**Note**: This behavior may vary based on the data source. For example, if you're working with a JDBC data source connected to a database that supports SQL-like operations, the `.cast` filter could potentially be pushed down to the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of operation where filter pushdown doesn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer_gt_50 = (\n",
    "    df_customers\n",
    "    .filter(F.col(\"age\").cast(\"int\") > 50)\n",
    ")\n",
    "df_customer_gt_50.show(5, False)\n",
    "df_customer_gt_50.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data-skew-explanation",
   "notebookOrigID": 4018749166498458,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
